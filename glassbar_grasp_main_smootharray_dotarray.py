#!/usr/bin/env python3
import sys
import signal
import atexit
sys.path.append("FoundationPose")
from estimater import *
from datareader import *
from dino_mask import get_mask_from_GD 
from qwen_mask import get_mask_from_qwen
from create_camera import CreateRealsense
import cv2
import numpy as np
# import open3d as o3d
import pyrealsense2 as rs
# import torch
import time, os, sys
import json
import threading
from datetime import datetime
import gc
import torch
# from ultralytics.models.sam import Predictor as SAMPredictor
from simple_api import SimpleApi, ForceMonitor, ErrorMonitor
from dobot_gripper import DobotGripper
from transforms3d.euler import euler2mat, mat2euler
from scipy.spatial.transform import Rotation as R
import queue
from spatialmath import SE3, SO3
from grasp_utils import normalize_angle, extract_euler_zyx, print_pose_info
from calculate_grasp_pose_from_object_pose import execute_grasp_from_object_pose, detect_dent_orientation
from camera_reader import CameraReader
import rospy
from std_msgs.msg import Float64MultiArray


camera = None
angle_camera = None
contact_camera = None
dobot = None
gripper = None
preview_running = None


def _cleanup_resources():
    """é‡Šæ”¾ç›¸æœºã€æœºæ¢°è‡‚å’Œçª—å£ç­‰èµ„æº"""
    global camera, angle_camera, contact_camera, dobot, preview_running
    
    # åœæ­¢ç›¸æœºé¢„è§ˆçº¿ç¨‹
    try:
        if preview_running:
            preview_running.clear()
            print("[æ¸…ç†] ç›¸æœºé¢„è§ˆçº¿ç¨‹å·²åœæ­¢")
    except Exception:
        pass
    
    try:
        if angle_camera and getattr(angle_camera, "cap", None):
            angle_camera.cap.release()
    except Exception:
        pass
    try:
        if contact_camera and getattr(contact_camera, "cap", None):
            contact_camera.cap.release()
    except Exception:
        pass
    try:
        if camera:
            camera.release()
    except Exception:
        pass
    try:
        if dobot:
            dobot.stop()
            dobot.disable_robot()
    except Exception:
        pass
    cv2.destroyAllWindows()


def _signal_handler(signum, frame):
    print("\n[ä¸­æ–­] ç”¨æˆ·ç»ˆæ­¢ç¨‹åº")
    _cleanup_resources()
    try:
        rospy.signal_shutdown("User interrupt")
    except Exception:
        pass
    sys.exit(0)


signal.signal(signal.SIGINT, _signal_handler)
atexit.register(_cleanup_resources)



# ---------- æ‰‹çœ¼æ ‡å®š ----------
def load_hand_eye_calibration(json_path="hand_eye_calibration.json"):
    """ä»JSONæ–‡ä»¶åŠ è½½æ‰‹çœ¼æ ‡å®šçŸ©é˜µ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    calibration = data['T_ee_cam']
    rotation_matrix = np.array(calibration['rotation_matrix'])
    translation_vector = calibration['translation_vector']
    return SE3.Rt(rotation_matrix, translation_vector, check=False)

# ä»ç›¸æœºåæ ‡ç³»åˆ°æœ«ç«¯æ‰§è¡Œå™¨åæ ‡ç³»çš„å˜æ¢çŸ©é˜µ
T_ee_cam = load_hand_eye_calibration()

# ---------- æœºæ¢°è‡‚ ----------
def init_robot():
    dobot = SimpleApi("192.168.5.1", 29999)
    dobot.clear_error()
    dobot.enable_robot()
    dobot.stop()
    # å¯åŠ¨åŠ›ä¼ æ„Ÿå™¨
    dobot.enable_ft_sensor(1)
    time.sleep(1)
    # åŠ›ä¼ æ„Ÿå™¨ç½®é›¶(ä»¥å½“å‰å—åŠ›çŠ¶æ€ä¸ºåŸºå‡†)
    dobot.six_force_home()
    time.sleep(1)
    # åŠ›ç›‘æ§çº¿ç¨‹
    # force_monitor = ForceMonitor(dobot)
    # force_monitor.start_monitoring()
    # error_monitor = ErrorMonitor(dobot)
    # error_monitor.start_monitoring()
    gripper = DobotGripper(dobot)
    gripper.connect(init=True)
    return dobot, gripper




if __name__ == "__main__":
    rospy.init_node('ros_test', anonymous=True)
    # åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„ä¿å­˜ç›®å½•
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    save_dir = os.path.join("record_images_during_grasp", timestamp)
    os.makedirs(save_dir, exist_ok=True)
    # print(f"å›¾åƒå°†ä¿å­˜åˆ°: {save_dir}")
    
    # åˆ›å»ºè§’åº¦æ•°æ®è®°å½•æ–‡ä»¶
    angle_log_path = os.path.join(save_dir, "angle_log.csv")
    with open(angle_log_path, 'w') as f:
        f.write("frame,timestamp,angle_z_deg,detected_angles,avg_angle\n")
    # print(f"è§’åº¦æ•°æ®å°†ä¿å­˜åˆ°: {angle_log_path}")
    
    camera = CreateRealsense("231522072272")                     
    # #? æ€ä¹ˆæ£€æŸ¥æ²¡æœ‰åï¼Ÿ
    # angle_camera = CameraReader(camera_id=11, init_camera=True)   #! ç”¨äºè§’åº¦æ£€æµ‹çš„USBç›¸æœº (id=11, æ˜¯ååŠ çš„)
    # contact_camera = CameraReader(camera_id=10, init_camera=True) #! ç”¨äºè§¦ç¢°æ£€æµ‹çš„USBç›¸æœº ï¼ˆid=10, æ˜¯åŸæ¥çš„ï¼‰
    
    # # å¯åŠ¨ç›¸æœºé¢„è§ˆçº¿ç¨‹
    # preview_running = threading.Event()
    # preview_running.set()
    # def _camera_preview_thread():
    #     """åå°çº¿ç¨‹ï¼šå®æ—¶æ˜¾ç¤ºä¸¤ä¸ªç›¸æœºç”»é¢"""
    #     # åœ¨çº¿ç¨‹å†…éƒ¨åˆ›å»ºçª—å£
    #     cv2.namedWindow("Angle Camera", cv2.WINDOW_NORMAL)
    #     cv2.namedWindow("Contact Camera", cv2.WINDOW_NORMAL)
    #     cv2.resizeWindow("Angle Camera", 640, 480)
    #     cv2.resizeWindow("Contact Camera", 640, 480)
        
    #     while preview_running.is_set():
    #         # è·å–è§’åº¦ç›¸æœºç”»é¢
    #         angle_frame = angle_camera.get_current_frame()
    #         if angle_frame is not None:
    #             cv2.imshow("Angle Camera", angle_frame)
            
    #         # è·å–æ¥è§¦ç›¸æœºç”»é¢
    #         contact_frame = contact_camera.get_current_frame()
    #         if contact_frame is not None:
    #             cv2.imshow("Contact Camera", contact_frame)
            
    #         # å¿…é¡»è°ƒç”¨waitKeyè®©çª—å£å“åº”
    #         key = cv2.waitKey(30)  # 30ms = çº¦33fps
    #         if key == ord('q'):
    #             print("ç”¨æˆ·æŒ‰'q'å…³é—­ç›¸æœºé¢„è§ˆ")
    #             preview_running.clear()
    #             break
    # preview_thread = threading.Thread(target=_camera_preview_thread, daemon=True)
    # preview_thread.start()
    # time.sleep(0.5)  # ç­‰å¾…çª—å£åˆ›å»º
    # print("ğŸ“¹ ç›¸æœºå®æ—¶é¢„è§ˆå·²å¯åŠ¨ (æŒ‰'q'å¯å…³é—­é¢„è§ˆçª—å£)")
    
    # mesh_file = "mesh/cube.obj"
    mesh_file = "mesh/thin_cube.obj"
    debug = 0
    debug_dir = "debug"
    set_logging_format()
    set_seed(0)
    mesh = trimesh.load(mesh_file)
    #? openscadçš„å•ä½æ˜¯mmï¼Œ ä½†æ˜¯è½¬ä¸ºobjæ–‡ä»¶åå•ä½åˆå˜æˆmï¼Œæ‰€ä»¥è¿˜æ˜¯éœ€è¦è½¬æ¢ï¼
    mesh.vertices /= 1000 #! å•ä½è½¬æ¢é™¤ä»¥1000
    # mesh.vertices /= 3
    to_origin, extents = trimesh.bounds.oriented_bounds(mesh)
    bbox = np.stack([-extents/2, extents/2], axis=0).reshape(2,3)

    # åˆå§‹åŒ–æœºæ¢°è‡‚
    dobot, gripper = init_robot()

    
    # åˆå§‹åŒ–è¯„åˆ†å™¨å’Œå§¿æ€ä¼˜åŒ–å™¨
    scorer = ScorePredictor() 
    refiner = PoseRefinePredictor()
    glctx = dr.RasterizeCudaContext()
    # åˆ›å»ºFoundationPoseä¼°è®¡å™¨
    est = FoundationPose(model_pts=mesh.vertices, model_normals=mesh.vertex_normals, mesh=mesh, scorer=scorer, refiner=refiner, debug_dir=debug_dir, debug=debug, glctx=glctx)
    logging.info("estimator initialization done")
    # è·å–ç›¸æœºå†…å‚
    cam_k = np.loadtxt(f'cam_K.txt').reshape(3,3)

    
    try:
        frame_count = 0
        last_valid_pose = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„pose
        last_valid_angle = None  # ä¿å­˜ä¸Šä¸€æ¬¡æœ‰æ•ˆçš„ROSè§’åº¦
        last_seen_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„ROSæ—¶é—´æˆ³ï¼ˆtracking_dataï¼‰
        last_seen_img_ts = None  # ä¸Šä¸€æ¬¡ä½¿ç”¨çš„å›¾åƒæ—¶é—´æˆ³
        last_valid_detected_angles = None  # ä¿å­˜ä¸Šä¸€æ¬¡æ£€æµ‹åˆ°çš„è§’åº¦åˆ—è¡¨
        last_valid_avg_angle = 0.0  # ä¿å­˜ä¸Šä¸€æ¬¡æ£€æµ‹åˆ°çš„å¹³å‡è§’åº¦
        
        while True:
            # è·å–å½“å‰å¸§
            # color = camera.get_frames()['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
            # depth = camera.get_frames()['depth']/1000
            # ir1 = camera.get_frames()['ir1']
            # ir2 = camera.get_frames()['ir2']
            frames = camera.get_frames()
            if frames is None:
                continue
            color = frames['color']  #get_framesè·å–å½“å‰å¸§çš„æ‰€æœ‰æ•°æ®ï¼ˆRGBã€æ·±åº¦ã€çº¢å¤–ç­‰ï¼‰
            depth = frames['depth']/1000
            ir1 = frames['ir1']
            ir2 = frames['ir2']

            color_path = os.path.join(save_dir, f"color_frame_{frame_count:06d}.png")
            print("befor foundation pose, color_shape: ", color.shape)
            cv2.imwrite(color_path, color)
            
            
            # æ¯éš”30å¸§è¿›è¡Œä¸€æ¬¡FoundationPoseæ£€æµ‹
            if frame_count % 15 == 0:
                #ä½¿ç”¨GroundingDINOè¿›è¡Œè¯­ä¹‰ç†è§£æ‰¾åˆ°ç‰©ä½“çš„ç²—ç•¥ä½ç½®ï¼ŒSAMè·å–ç‰©ä½“çš„ç›¸å¯¹ç²¾ç¡®æ©ç 
                mask = get_mask_from_qwen(color, "red stirring rod", model_path="/home/erlin/work/labgrasp/Qwen3-VL/Qwen3-VL-4B-Thinking", bbox_vis_path=os.path.join(save_dir, f"qwen_bbox_frame_{frame_count:06d}.png"))
                # mask = get_mask_from_GD(color, "red stirring rod")
                # mask = get_mask_from_GD(color, "Plastic dropper") 
                # mask = get_mask_from_GD(color, "long yellow bar")
                # mask = get_mask_from_GD(color, "long red bar")
                # print("mask_shape: ", mask.shape)
            
                cv2.imshow("mask", mask)
                cv2.imshow("color", color)
                pose = est.register(K=cam_k, rgb=color, depth=depth, ob_mask=mask, iteration=50)
                print(f"ç¬¬{frame_count}å¸§æ£€æµ‹å®Œæˆï¼Œpose: {pose}")
                center_pose = pose@np.linalg.inv(to_origin) #! è¿™ä¸ªæ‰æ˜¯ç‰©ä½“ä¸­å¿ƒç‚¹çš„Pose
                vis = draw_posed_3d_box(cam_k, img=color, ob_in_cam=center_pose, bbox=bbox)
                vis = draw_xyz_axis(color, ob_in_cam=center_pose, scale=0.1, K=cam_k, thickness=3, transparency=0, is_input_rgb=True)
                cv2.imshow('object 6D pose', vis[...,::-1])
    
                mask_path = os.path.join(save_dir, f"mask_frame_{frame_count:06d}.png")
                vis_path = os.path.join(save_dir, f"vis_frame_{frame_count:06d}.png")
                cv2.imwrite(mask_path, mask)
                cv2.imwrite(vis_path, vis[...,::-1])                

                cv2.waitKey(0) #waitKey(0) æ˜¯ä¸€ç§é˜»å¡
                input("break001") #inputä¹Ÿæ˜¯ä¸€ç§é˜»å¡
                print("break001")
                
                #? æ¸…ç†å†…å­˜ (è¿™ä¸ªæœ‰ç”¨å—ï¼Ÿ)
                torch.cuda.empty_cache()
                gc.collect()
 
                last_valid_pose = center_pose  # ä¿å­˜è¿™æ¬¡æ£€æµ‹çš„ç»“æœ
            else:
                # ä½¿ç”¨ä¸Šä¸€æ¬¡æ£€æµ‹çš„ç»“æœ
                center_pose = last_valid_pose
                # print(f"ç¬¬{frame_count}å¸§ä½¿ç”¨ä¸Šæ¬¡æ£€æµ‹ç»“æœ")
            

            print("center_pose_object: ", center_pose) 
            
            frame_count += 1

            if center_pose is not None:
                break

    except KeyboardInterrupt:
        print("\n[ç”¨æˆ·ä¸­æ–­] æ”¶åˆ°ç»ˆæ­¢ä¿¡å·")
    finally:
        cv2.destroyAllWindows()
        # dobot.disable_robot()


    key = cv2.waitKey(1)
    # if key == ord('q'):  # æŒ‰qé€€å‡º
    #     break
    # elif key == ord('a'):  # æŒ‰aæ‰§è¡ŒæŠ“å–
    #
    # init_position = 10
    # gripper.control(position=init_position, force=80, speed=10)


    #? æ€ä¹ˆæ£€æŸ¥æ²¡æœ‰åï¼Ÿ
    angle_camera = CameraReader(camera_id=11, init_camera=True)   #! ç”¨äºè§’åº¦æ£€æµ‹çš„USBç›¸æœº (id=11, æ˜¯ååŠ çš„)
    contact_camera = CameraReader(camera_id=10, init_camera=True) #! ç”¨äºè§¦ç¢°æ£€æµ‹çš„USBç›¸æœº ï¼ˆid=10, æ˜¯åŸæ¥çš„ï¼‰


    # å°†center_poseè½¬æ¢ä¸ºnumpyæ•°ç»„
    center_pose_array = np.array(center_pose, dtype=float)
    
    # ------ä½¿ç”¨å°è£…å‡½æ•°æ‰§è¡ŒæŠ“å–------
    # é…ç½®æŠ“å–å‚æ•°
    z_xoy_angle = 0 # ç‰©ä½“ç»•zè½´æ—‹è½¬è§’åº¦
    vertical_euler = [-180, 0, -90]  # å‚ç›´å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„rx, ry, rz
    grasp_tilt_angle = 30  #  ç”±å‚ç›´å‘ä¸‹æŠ“å–æ—‹è½¬ä¸ºæ–œç€å‘ä¸‹æŠ“å–çš„graspå§¿æ€çš„æ—‹è½¬è§’åº¦ï¼š åŠ äº†30åº¦ä¼šæœå¤–æ—‹è½¬
    z_safe_distance= 46  #zæ–¹å‘çš„ä¸€ä¸ªå®‰å…¨è·ç¦»ï¼Œä¹Ÿæ˜¯ä¸ºäº†æŠ“å–ç‰©ä½“é ä¸Šçš„éƒ¨åˆ†ï¼Œå¯çµæ´»è°ƒæ•´
    
    # è°ƒç”¨å°è£…å‡½æ•°æ‰§è¡ŒæŠ“å–
    success, T_base_ee_ideal = execute_grasp_from_object_pose(
        center_pose_array=center_pose_array,
        dobot=dobot,
        gripper=gripper,
        T_ee_cam=T_ee_cam,
        z_xoy_angle=z_xoy_angle,
        vertical_euler=vertical_euler,
        grasp_tilt_angle=grasp_tilt_angle,
        angle_threshold=10.0,
        T_tcp_ee_z= -0.16, 
        T_safe_distance= 0.00, #å¯çµæ´»è°ƒæ•´
        z_safe_distance=z_safe_distance,
        gripper_close_pos=15,
        verbose=True
    )
    
    pose_now = dobot.get_pose()
    x_adjustment = 10
    z_adjustment = 50
    dobot.move_to_pose(pose_now[0]+x_adjustment, pose_now[1], pose_now[2]+z_adjustment, pose_now[3], pose_now[4], pose_now[5], speed=7, acceleration=1) 


#-----------å¼€å§‹æ£€æµ‹ç»ç’ƒæ£’æ–¹å‘-------------------------------------------------------
    print("\n" + "="*60)
    print("ğŸ” å¼€å§‹æ£€æµ‹ç»ç’ƒæ£’æ–¹å‘...")
    print("="*60)
    
    detected_angles = None
    avg_angle = 0.0
    detection_attempts = 0
    
    while True:
        detection_attempts += 1

        raw_image = angle_camera.get_current_frame()
        if raw_image is None:
            print(f"ç¬¬{detection_attempts}æ¬¡å°è¯•: ç­‰å¾…ç›¸æœºæ•°æ®...")
            time.sleep(0.1)
            continue
        img_timestamp = time.time()

        print(f"\nğŸ“· ç¬¬{detection_attempts}æ¬¡å°è¯•: æ£€æµ‹æ–°åŸå§‹å›¾åƒæ–¹å‘ (æ—¶é—´æˆ³: {img_timestamp:.2f})")
        detected_angles, avg_angle = detect_dent_orientation(raw_image, save_dir=save_dir)

        if detected_angles:
            last_valid_detected_angles = detected_angles
            last_valid_avg_angle = avg_angle
            last_seen_img_ts = img_timestamp
            print(f"æˆåŠŸæ£€æµ‹åˆ°ç‰©ä½“æœå‘è§’åº¦: {detected_angles}, å¹³å‡: {avg_angle:.2f}Â°, ç»å¯¹å€¼: {abs(avg_angle):.2f}Â°")
            print("="*60)
            break
        else:
            print("å½“å‰å›¾åƒæœªæ£€æµ‹åˆ°æ˜æ˜¾æ–¹å‘ç‰¹å¾ï¼Œç»§ç»­ç­‰å¾…...")
            time.sleep(0.1)

        # å¯é€‰ï¼šæœ€å¤§å°è¯•æ¬¡æ•°é™åˆ¶
        if detection_attempts >= 100:
            print(" è­¦å‘Š: è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°(100æ¬¡)ï¼Œä½¿ç”¨é»˜è®¤è§’åº¦")
            detected_angles = []
            avg_angle = 0.0
            break


#-----------å¼€å§‹è°ƒæ•´ç»ç’ƒæ£’å§¿æ€-------------------------------------------------------

    print("å¼€å§‹è°ƒæ•´ç»ç’ƒæ£’å§¿æ€è‡³å‚ç›´æ¡Œé¢å‘ä¸‹")
    pose_now = dobot.get_pose()
    delta_ee = abs(avg_angle) - grasp_tilt_angle
    #éœ€è¦è®©tcpæœå¤–æ—‹è½¬ï¼› grasp_tilt_angleä¸ºæ­£å€¼æ—¶ï¼Œtcpä¼šæœå¤–æ—‹è½¬ã€‚
    pose_target = [pose_now[0]+15, pose_now[1], pose_now[2], pose_now[3]+delta_ee, pose_now[4], pose_now[5]]
    dobot.move_to_pose(pose_target[0], pose_target[1], pose_target[2], pose_target[3], pose_target[4], pose_target[5], speed=12, acceleration=1)
    

    wait_rate = rospy.Rate(1.0 / 12.0)  
    wait_rate.sleep()
    
    # éªŒè¯æ˜¯å¦åˆ°è¾¾ç›®æ ‡ä½ç½®
    pose_after_adjust = dobot.get_pose()
    print(f"æ£€æŸ¥å§¿æ€è°ƒæ•´æ˜¯å¦å®Œæˆ: Rx={pose_after_adjust[3]:.2f}Â° (ç›®æ ‡: {pose_target[3]:.2f}Â°)")



#-----------å¼€å§‹æ£€æµ‹ç»ç’ƒæ£’æ˜¯å¦è§¦ç¢°åˆ°æ¡Œé¢-------------------------------------------------------
    print("\nå¼€å§‹ç›‘æµ‹ç»ç’ƒæ£’ä¸æ¡Œé¢æ¥è§¦...")

    gray_debug_dir = os.path.join(save_dir, "gray_images_debug")
    os.makedirs(gray_debug_dir, exist_ok=True)
    print(f"ç°åº¦å›¾å°†ä¿å­˜åˆ°: {gray_debug_dir}")

    sample_interval = 0.1  # ç§’
    move_step = 3          # mm
    max_steps = 700
    change_threshold = 3 #0.06% å˜åŒ–çµæ•åº¦ 

    rate = rospy.Rate(1.0 / sample_interval)
    rate.sleep()
    # rospy.sleep(sample_interval)
    frame_before = None
    while frame_before is None:
        initial_frame = contact_camera.get_current_frame()
        if initial_frame is not None:
            frame_before = initial_frame
        else:
            print("ç­‰å¾…åˆå§‹å›¾åƒ...")
            rospy.sleep(sample_interval)

    print("å·²è·å–åˆå§‹å›¾åƒ")
    pose_current = dobot.get_pose()

    for step in range(max_steps):
        wait = rospy.Rate(33)  
        wait.sleep()
        # åŠ¨ä½œå‰å¸§
        frame_data_before = contact_camera.get_current_frame()
        if frame_data_before is None:
            print(f"  æ­¥éª¤ {step+1}: ç­‰å¾…åŠ¨ä½œå‰å›¾åƒ...")
            rospy.sleep(sample_interval)
            continue
        frame_before = frame_data_before

        # å‘ä¸‹ç§»åŠ¨ä¸€å°æ­¥
        pose_current[2] -= move_step
        dobot.move_to_pose(
            pose_current[0], pose_current[1], pose_current[2],
            pose_current[3], pose_current[4], pose_current[5],
            speed=5, acceleration=1
        )

        # ç­‰å¾…å¹¶æŠ“å–åŠ¨ä½œåçš„æ–°å¸§
        frame_after = None
        has_change = False
        #è¿ç»­é«˜é¢‘é‡‡æ ·æ£€æµ‹
        for _ in range(20): #0.1*20 = 2s
            rate.sleep()
            candidate_frame = contact_camera.get_current_frame()
            if candidate_frame is not None:
                frame_after = candidate_frame

                has_change = contact_camera.has_significant_change(
                    frame_before, frame_after,
                    change_threshold=change_threshold,
                    pixel_threshold=2,
                    min_area=2,
                    save_dir=gray_debug_dir,
                    step_num=step
                )

                if has_change:
                    break
            
                # break

        if frame_after is None:
            print(f"  æ­¥éª¤ {step+1}: æœªæ”¶åˆ°æ–°å›¾åƒï¼Œç»§ç»­ç­‰å¾…...")
            continue


        if has_change:
            print(f"æ£€æµ‹åˆ°æ˜¾è‘—å˜åŒ–ï¼ç»ç’ƒæ£’å¯èƒ½å·²æ¥è§¦æ¡Œé¢ (æ­¥æ•°: {step+1}, ä¸‹é™: {(step+1)*move_step}mm)")
            break

        print(f"  æ­¥éª¤ {step+1}/{max_steps}: æœªæ£€æµ‹åˆ°æ¥è§¦ï¼Œç»§ç»­ä¸‹é™...")
    else:
        print("è¾¾åˆ°å‚ç›´å‘ä¸‹æœ€å¤§ç§»åŠ¨è·ç¦»ï¼Œæœªæ£€æµ‹åˆ°æ˜æ˜¾å˜åŒ–")

    print("ç»ç’ƒæ£’ä¸‹é™æ£€æµ‹å®Œæˆ\n")

        
    # å¯é€‰ï¼šè¿”å›homeä½ç½®ï¼ˆæ ¹æ®éœ€è¦å–æ¶ˆæ³¨é‡Šï¼‰
    # dobot.move_to_pose(435.4503, 281.809, 348.9125, -179.789, -0.8424, 14.4524, speed=9)


